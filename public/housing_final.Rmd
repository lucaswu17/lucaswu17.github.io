---
title: "STAT 440: House Prices Modelling"
author: "Team: Yifan(Lucas) Wu; Kaggle Team Name: 'Let's go 440' "
date: "September 27, 2016"
output: 
blogdown::html_page:
      toc: yes
      toc_float: yes
      toc_depth: 3
      collapsed: FALSE
    dev: "svg"
---

```{r setup, include=FALSE}
## install and load packages
# ipak <- function(pkg){
#   new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
#   if (length(new.pkg)) 
#     install.packages(new.pkg, dependencies = TRUE)
#   sapply(pkg, require, character.only = TRUE)
# }

# packages <- c("dplyr","ggplot2", "plyr", "moments", "mice",
#               "VIM" ,"corrplot","car","caret","RColorBrewer",
#               "glmnet", "randomForest","xgboost","data.table" ,"Matrix","xgboost", 
#           "tidyverse","pROC","dummies","Metrics","kernlab","mlbench",
#            "Hmisc",  "xtable", "knitr", "forcats")
# ipak(packages)
library(pacman)
pacman::p_load("dplyr","ggplot2", "plyr", "moments", "mice",
              "VIM" ,"corrplot","car","caret","RColorBrewer",
              "glmnet" ,"Matrix", "tidyverse","pROC","dummies","Metrics","mlbench",
           "Hmisc",  "xtable", "knitr", "forcats")
```

```{r useful_functions, include=FALSE}
# Evaluation metric (RMSE of log prices)
eval_metric <- function(predicted_sales_price, actual_sales_price){
  sqrt(mean(((predicted_sales_price) - log(actual_sales_price))^2))
}
```

## Introduction

We are given a data science competition, House Prices: Advanced Regression Techniques, from Kaggle. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges us to predict the final price of each home.
More info can be found at: https://www.kaggle.com/c/house-prices-advanced-regression-techniques

#### Load in and explore data

```{r load_data}
house_prices_data <- read.csv('data/train.csv', stringsAsFactors = FALSE)
house_prices_data_test <- read.csv('data/test.csv', stringsAsFactors = FALSE)
```

The training data set of housing consists of `r nrow(house_prices_data)` rows and `r ncol(house_prices_data)` columns.

## Method 1: Stepwise Regression 

#### Standing on the Shoulder of Giants

Credits to the idea of Sung Ha which scored 0.12794 on Kaggle's leaderboard! However, the stepwise method has a major drawback which requires us to convert all the categorical variables into numerical variables to fix the problem of unseen level in the test set.
	
```{r Sung_code}
train_sung <- house_prices_data %>% mutate_if(is.character, as.factor)

n_na <-sapply(train_sung,function(y)length(which(is.na(y)==T)))
n_na.df<- data.frame(var=colnames(train_sung),num_NA=n_na)

train_sung <- train_sung[,!(names(train_sung)%in%c("Id","Alley","PoolQC","Fence","MiscFeature"))]

num<-sapply(train_sung,is.numeric)
num<-train_sung[,num]

for(i in 1:76){
  if(is.factor(train_sung[,i])){
    train_sung[,i]<-as.integer(train_sung[,i])
  }
}
train_sung[is.na(train_sung)]<-0
num[is.na(num)]<-0

train.train<- train_sung[1:floor(length(train_sung[,1])*0.8),]
train.train$SalePrice <- log(train.train$SalePrice)
train.test<- train_sung[(length(train.train[,1])+1):1460,]
train.test$SalePrice <- log(train.test$SalePrice)

lm.train.train <- lm(SalePrice~.,train.train)
# summary(lm.train.train)
step.lm.train.train <- step(lm.train.train,trace = 0)
# summary(step.lm.train.train)
lm.train.train <- lm(SalePrice~.,step.lm.train.train$model)
# plot(lm.train.train)

# Load and predict on test set
test_sung <- house_prices_data_test %>% mutate_if(is.character, as.factor)

n_na <-sapply(test_sung,function(y)length(which(is.na(y)==T)))
n_na.df<- data.frame(var=colnames(test_sung),num_NA=n_na)

test_sung <- test_sung[,!(names(test_sung)%in%c("Id","Alley","PoolQC","Fence","MiscFeature"))]

num<-sapply(test_sung,is.numeric)
num<-test_sung[,num]

for(i in 1:75){
  if(is.factor(test_sung[,i])){
    test_sung[,i]<-as.integer(test_sung[,i])
  }
}
test_sung[is.na(test_sung)]<-0
num[is.na(num)]<-0

sung.predicted_values <- exp(predict(lm.train.train, newdata = test_sung))
```

#### Validation

LOOCV for the stepwise model.

```{r cross_validation}
# leave-one-out cross-validation
out_of_sample_prediction <- rep(NA, nrow(train_sung))
train_sung$SalePrice <- log(train_sung$SalePrice)
for(data_point in 1:nrow(train_sung)){
  # Fit model on data with point left out
  # lm_model_loo <- lm(SalePrice ~ OverallQual + GrLivArea , data = train[-data_point, ])
  out_of_sample_prediction[data_point] <- (predict(lm.train.train, newdata = train_sung[data_point, ]))
}
```


```{r evaluate_cv}
out_of_sample_prediction[out_of_sample_prediction < 0] <- 100
eval_metric(out_of_sample_prediction, house_prices_data$SalePrice)
```

The final LOOCV score is `r eval_metric(out_of_sample_prediction, house_prices_data$SalePrice)`.

## Method 2: Shrinkage Methods

A pretty good result from stepwise regression indicates that there should be room for improvement by employing regularized regression such as Ridge Regression or LASSO.

## Explorary Data Analysis

```{r read_in}
house_prices_data_test$SalePrice <- -999
full <- rbind(house_prices_data,house_prices_data_test)

full <- full %>% mutate_if(is.character, as.factor)

factor_columns <- names(which(sapply(full, class) == 'factor'))

non_factor_columns <- names(which(sapply(full, class) != 'factor'))
non_factor_columns <- non_factor_columns[!non_factor_columns %in% c("Id","SalePrice")]
```

If we take a look at the missing values in this data set, there are `r sum(is.na(house_prices_data))` missing values in the training set and `r sum(is.na(house_prices_data_test))` in the test set.

```{r fig.width=8, fig.height=6, warning=FALSE}
# display the pattern of missing values
mice_plot <- aggr(full, col=c('navyblue','yellow'),
                  numbers=T, sortVars=F,  
                  labels=names(full), cex.axis=.8,
                  gap=3, ylab=c("Missing data","Pattern"))
```


We can further investigate how many missing values in each column in the training set and test set:
```{r missing_counts}
missing_full <- sapply(full, function(x) sum(is.na(x)))
missing_full[missing_full!=0]
```

Correlation plot
```{r cor_plot,fig.width=10, fig.height=10, warning=FALSE}
M<-cor(na.omit(full[non_factor_columns]))
corrplot(M, type="upper", order="hclust",col=brewer.pal(n=8, name="PuOr"))
```

## Feature Engineering

1. Replace NA in numeric variables with their mean
2. Replace NA in categorical variables with zero
3. Generate new variables, Age, OverallQual Square and GrLivArea Square
4. Log-transform skewed variables
5. Trying out features of Maude

```{r}
house_prices_data <- read.csv('data/train.csv', stringsAsFactors = FALSE)
house_prices_data_test <- read.csv('data/test.csv', stringsAsFactors = FALSE)
house_prices_data <- house_prices_data[-c(524,692,1183,1299), ]
house_prices_data_test$SalePrice <- -999
full <- rbind(house_prices_data,house_prices_data_test)

missing_full_after <- sapply(full, function(x) sum(is.na(x)))
missing_full_after[missing_full_after!=0]

trainTest <- full
cat_var <- which(sapply( trainTest, class ) == "character")
trainTest[,cat_var] <- replace(trainTest[,cat_var], 
                               is.na(trainTest[,cat_var]), 'miss')
# turn into factors
# trainTest[,cat_var] <- as.data.frame(unclass(trainTest[,cat_var]))
trainTest <- trainTest %>% mutate_if(is.character, as.factor)

factor_columns <- names(which(sapply(trainTest, class) == 'factor'))
factor_columns

non_factor_columns <- names(which(sapply(trainTest, class) != 'factor'))
# get rid of Id 
non_factor_columns <- non_factor_columns[!non_factor_columns %in% c("Id","SalePrice")]
non_factor_columns

one <- c(20,30,40,45,50,120,150) 
two <- c(60,70,75,160) 
three <- c(80,85,90,180,190)


trainTest$MSSubClass[trainTest$MSSubClass %in% one] <- 1 
trainTest$MSSubClass[trainTest$MSSubClass %in% two] <- 2 
trainTest$MSSubClass[trainTest$MSSubClass %in% three] <- 3 

all_data <- trainTest
for (i in 1:length(non_factor_columns)){
  if (skewness(all_data[non_factor_columns[i]],na.rm = TRUE) > 0.75) {
    all_data[non_factor_columns[i]] <- log(all_data[non_factor_columns[i]]+1)
  }
}

trainTest <- all_data

## ----features credits to Maude
trainTest$LandContour <- fct_relevel(trainTest$LandContour, "Lvl", "Bnk", "HLS", "Low")
trainTest$ExterQual   <- fct_relevel(trainTest$ExterQual, 'Fa', 'TA', 'Gd', 'Ex')
trainTest$ExterCond   <- fct_relevel(trainTest$ExterCond, 'Po','Fa', 'TA', 'Gd', 'Ex')
trainTest$BsmtQual   <- fct_relevel(trainTest$BsmtQual, 'miss','Fa', 'TA', 'Gd', 'Ex')
trainTest$BsmtCond   <- fct_relevel(trainTest$BsmtCond, 'miss','Po','Fa', 'TA', 'Gd')
trainTest$BsmtExposure   <- fct_relevel(trainTest$BsmtExposure, 'miss','No','Mn', 'Av', 'Gd')
trainTest$BsmtFinType1   <- fct_relevel(trainTest$BsmtFinType1, 'miss','Unf','LwQ', 'Rec', 'BLQ',
                                        'ALQ','GLQ')
trainTest$BsmtFinType2   <- fct_relevel(trainTest$BsmtFinType2, 'miss','Unf','LwQ', 'Rec', 'BLQ',
                                        'ALQ','GLQ')
trainTest$HeatingQC   <- fct_relevel(trainTest$HeatingQC,'Po','Fa', 'TA', 'Gd', 'Ex')
trainTest$Electrical   <- fct_relevel(trainTest$Electrical,'FuseP','FuseF', 'Mix','FuseA', 'SBrkr')
trainTest$KitchenQual   <- fct_relevel(trainTest$KitchenQual, 'miss','Fa', 'TA', 'Gd', 'Ex')

trainTest$Functional   <- fct_collapse(trainTest$Functional, Min=c('miss', 'Min1', 'Min2'),
                                       High=c('Sev', 'Maj2', 'Maj1','Mod'),
                                       Type='Typ')
trainTest$Functional   <- fct_relevel(trainTest$Functional, 'Type','Min','High')
trainTest$FireplaceQu  <- fct_relevel(trainTest$FireplaceQu,'miss', 'Po','Fa', 'TA', 'Gd', 'Ex')
trainTest$GarageFinish  <- fct_relevel(trainTest$GarageFinish,'miss', 'Unf','RFn', 'Fin')
trainTest$GarageQual   <- fct_relevel(trainTest$GarageQual, 'miss','Po','Fa', 'TA', 'Gd', 'Ex')

trainTest$GarageCond   <- fct_collapse(trainTest$GarageCond, Po=c('miss', 'Po'),
                                       Fa='Fa',
                                       TA='TA',
                                       Gd=c('Gd', 'Ex'))
trainTest$GarageCond   <- fct_relevel(trainTest$GarageCond, 'Po','Fa', 'TA', 'Gd')

trainTest$Neighborhood <- fct_collapse(trainTest$Neighborhood, 
                                       'A'=c("MeadowV","IDOTRR","BrDale","OldTown",
                                             "Edwards","BrkSide"),
                                       'B'=c("Sawyer","Blueste","SWISU","NAmes",
                                             "NPkVill", "Mitchel"),
                                       'C'=c("SawyerW","Gilbert","NWAmes","Blmngtn",
                                             "CollgCr","ClearCr","Crawfor","Veenker",
                                             "Somerst","Timber"),
                                       'D'=c("StoneBr","NoRidge","NridgHt"))


trainTest$Neighborhood <- fct_relevel(trainTest$Neighborhood, "A", "B", "C", "D")

trainTest <-trainTest %>%
  mutate(regShape=if_else( LotShape == 'Reg',1, 0)) %>%
  mutate(regSlope=if_else( LandSlope == 'Gtl',1, 0)) %>%              
  mutate(agri=    if_else( MSZoning == 'A',  1, 0)) %>%
  mutate(comm=    if_else( MSZoning == 'C',  1, 0)) %>%
  mutate(industry=if_else( MSZoning == 'I',  1, 0)) %>%
  mutate(resDense=if_else( MSZoning == 'RH' |
                             MSZoning == 'RM', 1, 0)) %>%             
  mutate(resLow=  if_else( MSZoning == 'RL' |
                             MSZoning == 'RP', 1, 0)) %>%
  mutate(industry=if_else( MSZoning == 'FV', 1, 0)) %>%
  
  mutate(spring=if_else( MoSold == 3 |
                           MoSold == 4 |
                           MoSold == 5, 1, 0)) %>%
  mutate(summer=if_else( MoSold == 6 |
                           MoSold == 7 |
                           MoSold == 8, 1, 0)) %>%
  mutate(fall=if_else(   MoSold == 9 |
                           MoSold == 10 |
                           MoSold == 11, 1, 0)) %>% 
  mutate(winter=if_else( MoSold == 12 |
                           MoSold == 1 |
                           MoSold == 2, 1, 0)) %>%                                                  
  
  mutate( threeSides=if_else( LotConfig == 'CulDSac'  |
                                LotConfig == 'FR3', 1, 0)) %>%              
  mutate( posFeat=if_else( Condition1 == 'PosA' |
                             Condition1 == 'PosN' |
                             Condition2 == 'PosA' |
                             Condition2 == 'PosN', 1, 0)) %>%
  mutate(year =if_else( YearBuilt == YearRemodAdd, YearBuilt, YearRemodAdd)) %>%
  mutate(barnRoof=if_else( RoofStyle  == 'Gambrel', 1, 0)) %>%
  
  # don't include categories with < 20 
  mutate(asbestos=if_else( Exterior1st == 'AsbShng' |
                             Exterior2nd == 'AsbShng', 1, 0)) %>%  
  mutate(brick=if_else(    Exterior1st == 'BrkFace' |
                             Exterior2nd == 'BrkFace', 1, 0)) %>%
  mutate(cement=if_else(   Exterior1st == 'CemntBd' |
                             Exterior2nd == 'CemntBd', 1, 0)) %>%
  mutate(board=if_else(    Exterior1st == 'HdBoard' |
                             Exterior2nd == 'HdBoard', 1, 0)) %>%                                                   
  mutate(metalSid=if_else( Exterior1st == 'MetalSd' |
                             Exterior2nd == 'MetalSd', 1, 0)) %>%
  mutate(plywood=if_else(  Exterior1st == 'Plywood' |
                             Exterior2nd == 'Plywood', 1, 0)) %>% 
  mutate(vinyl =if_else(   Exterior1st == 'VinylSd' |
                             Exterior2nd == 'VinylSd', 1, 0)) %>% 
  mutate(stucco=if_else(   Exterior1st == 'Stucco' |
                             Exterior2nd == 'Stucco', 1, 0)) %>%
  mutate(sidingEx=if_else( Exterior1st == 'Wd Sdng' |
                             Exterior2nd == 'Wd Sdng', 1, 0)) %>%
  mutate(shingEx=if_else(  Exterior1st == 'WdShing' |
                             Exterior2nd == 'WdShing', 1, 0)) %>%
  mutate(brickVen=if_else( MasVnrType == 'BrkCmn'  |
                             MasVnrType == 'BrkFace', 1, 0)) %>%
  mutate(stoneVen =if_else(MasVnrType == 'Stone', 1, 0)) %>%      
  mutate(concreteF =if_else(Foundation == 'PConc', 1, 0)) %>%               
  mutate(garAttached = if_else(GarageType == 'Attchd' |
                                 GarageType == 'BuiltIn'|
                                 GarageType == 'Basment', 1, 0, 0)) %>%
  
  # note: try to figure out how to deal with illegal characters without
  # changing the data file
  mutate(porchSF=WoodDeckSF + OpenPorchSF + EnclosedPorch + 
           X3SsnPorch + ScreenPorch) %>%
  mutate(totalSF = GrLivArea + TotalBsmtSF)%>%
  mutate(bath = BsmtFullBath + FullBath + (0.5*(BsmtHalfBath + HalfBath))) %>%
  mutate(saleNorm =if_else(SaleCondition == 'Normal', 1, 0))  %>%
  as.data.frame()


full <- trainTest


full <- full %>% mutate(Age = YrSold - YearBuilt,
                        GrLivArea_Square = GrLivArea*GrLivArea,
                        OverallQualF = factor(ifelse(OverallQual>8,1,0)),
                        OverallCondF = factor(ifelse(OverallCond<5,1,0)),
                        BsmtQualF   =  factor(BsmtQual)
)


all_data <- full
all_data <- all_data %>% select(-Alley,-MiscFeature,-PoolQC,-Fence)

missing_full_after <- sapply(all_data, function(x) sum(is.na(x)))
missing_full_after[missing_full_after!=0]

feature_classes <- sapply(names(all_data),function(x){class(all_data[[x]])})
categorical_feats <- names(feature_classes[feature_classes == "factor"])
numeric_feats <-names(feature_classes[feature_classes != "factor"])

numeric_df <- all_data[numeric_feats]

for (x in numeric_feats) {
  mean_value <- mean(full[[x]],na.rm = TRUE)
  all_data[[x]][is.na(all_data[[x]])] <- mean_value
}


dummies <- dummyVars(~.,all_data[categorical_feats])
categorical_1 <- predict(dummies,all_data[categorical_feats])
categorical_1[is.na(categorical_1)] <- 0  


all_data <- cbind(all_data[numeric_feats],categorical_1)
# create data for training and test
X_train <- all_data[1:nrow(house_prices_data),]
X_test <- all_data[(nrow(house_prices_data)+1):nrow(all_data),]
y <- log(house_prices_data$SalePrice+1)
# y <- house_prices_data$SalePrice
# y <- log(house_prices_data$SalePrice/house_prices_data$GrLivArea)
X_train$SalePrice <- NULL
X_test$SalePrice <- NULL

X_train$Id <- NULL
X_test$Id <- NULL
x_train <- as.matrix(X_train)
x_test <-  as.matrix(X_test)
```


## Model Fitting - LASSO

There are too many variables after transformation so I decided to use LASSO to perform both variable selection and regularization to enhance the prediction accuracy and interpretability of my model.

```{r fit_model,fig.width=8, fig.height=6, warning=FALSE}
set.seed(2016)
cv1=cv.glmnet(x_train,y,nfolds=10,alpha=1)
fit = glmnet(x_train, y,alpha = cv1$lambda.min)
plot(fit, xvar = "lambda", label = TRUE)
#plot(fit, xvar = "dev", label = TRUE)
# coef(cv1)[coef(cv1)!=0]
predicted_values1 <- exp(predict(cv1,s=cv1$lambda.min,newx=x_test))
```



## Create Submission File

Ensemble the predictions from Stepwise Regression and LASSO to get final predictions.

```{r load_test_ data}
# Predict on test set and fixed some outliers
summary(predicted_values1)
summary(sung.predicted_values)
predicted_values1[661] <- sung.predicted_values[661]
sung.predicted_values[1090] <- predicted_values1[1090]
predicted_values <- (sung.predicted_values + predicted_values1)/2

# Create file for submission
submission_matrix <- data.frame(cbind(house_prices_data_test$Id, predicted_values))
colnames(submission_matrix) = c('Id', 'SalePrice')
submission_matrix$SalePrice <- round(submission_matrix$SalePrice)
#submission_matrix$SalePrice <- pmax(100, submission_matrix$SalePrice)

# Write submission file
write.csv(submission_matrix, file='submission_file.csv', row.names = FALSE)
```